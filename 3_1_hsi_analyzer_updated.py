# -*- coding: utf-8 -*-
"""3.1_HSI_Analyzer_Updated.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1J_siA5_OskyUFUIPdoQ0NAl1PKSe4KAD
"""
# %%
import pandas as pd
import numpy as np
import random
from matplotlib import pyplot as plt
from matplotlib.colors import Normalize
from chemotools.feature_selection import RangeCut
from chemotools.baseline import LinearCorrection
from chemotools.derivative import SavitzkyGolay
from chemotools.smooth import MeanFilter
from sklearn.cross_decomposition import PLSRegression
from sklearn.model_selection import GridSearchCV, KFold, cross_val_score
from sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler
from sklearn.pipeline import make_pipeline
from sklearn.svm import SVR
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import r2_score, mean_squared_error
from sklearn.neural_network import MLPRegressor
from sklearn.neighbors import KNeighborsRegressor
from xgboost import XGBRegressor
import seaborn as sns
from sklearn.model_selection import RandomizedSearchCV

"""# Loading the Data"""

specta_file_path = r"E:/ufl/Aim/Aim_1/HSI_spectra_124_Updated.xlsx"
spectra = pd.read_excel(specta_file_path)
# C:\Users\Robert\PHD\Python\My work\spectra.xlsx
# G:\YUWANGDATA\PHD\Python\My work\spectra.xlsx
label_file_path = r"E:/ufl/Aim/Aim_1/HSI_label_Brix_124_Updated.xlsx"
label = pd.read_excel(label_file_path)
# C:\Users\Robert\PHD\Python\My work\label.xlsx
# G:\YUWANGDATA\PHD\Python\My work\label.xlsx
print(f"Number of samples: {spectra.shape[0]}")
print(f"Number of wavenumbers: {spectra.shape[1]}")

label.describe()

# Convert the spectra pandas.DataFrame to numpy.ndarray
spectra_np = spectra.to_numpy()
# Convert the wavenumbers pandas.columns to numpy.ndarray
wavenumbers = spectra.columns.to_numpy(dtype=np.float64)

# Convert the label pandas.DataFrame to numpy.ndarray
label_np = label.to_numpy()

# Creating this to visualize the spectra.

def plot_spectra(spectra: np.ndarray, wavenumbers: np.ndarray, label: np.ndarray, chartTitle = "Citrus Reflectance"):
    # Define a colormap
    cmap = plt.get_cmap("jet")

    # Define a normalization function to scale glucose concentrations between 0 and 1
    normalize = Normalize(vmin=label.min(), vmax=label.max())
    colors = [cmap(normalize(value)) for value in label]

    # Plot the spectra
    fig, ax = plt.subplots(figsize=(10, 4))
    for i, row in enumerate(spectra):
        ax.plot(wavenumbers, row, color = colors[i])

    # Add a colorbar
    sm = plt.cm.ScalarMappable(cmap=cmap, norm=normalize)
    sm.set_array([])
    fig.colorbar(sm, ax=ax, label='Brix')

    # Add labels
    ax.set_xlabel('Wavelength (nm)')
    ax.set_ylabel('Reflectance')
    ax.set_title(chartTitle)

    plt.show()

plot_spectra(spectra_np, wavenumbers, label_np)

"""# Preprocessing

## Savitzky-Golay
"""

# create a pipeline that scales the data
# Savitzky-Golay filtering to smooth high-frequency noise
spectra_np_log = np.log1p(spectra_np)
SG_preprocessing = make_pipeline(
    RangeCut(start=400, end=1000, wavenumbers=wavenumbers),
    LinearCorrection(),
    SavitzkyGolay(window_size=20, polynomial_order=3, derivate_order=1),
    StandardScaler(with_std=False)
)

# SG_spectra_preprocessed = SG_preprocessing.fit_transform(spectra_np_log)
SG_spectra_preprocessed = SG_preprocessing.fit_transform(spectra_np)

# get the wavenumbers after the range cut
wavenumbers_cut = SG_preprocessing.named_steps['rangecut'].wavenumbers_


# plot the preprocessed spectra
plot_spectra(SG_spectra_preprocessed, wavenumbers_cut, label_np, chartTitle="Savitzky-Golay Reflectance")


''' change parameter '''
# preprocess = make_pipeline(
#     RangeCut(start=400, end=1000, wavenumbers=wavenumbers),
#     LinearCorrection(),
#     StandardScaler(with_std=False)
#     # MinMaxScaler()

# )
# spectra_np = np.log1p(spectra_np)
# preprocessed_data_without_sg = preprocess.fit_transform(spectra_np)
# sg_filter = SavitzkyGolay(window_size=20, polynomial_order=2, derivate_order=0)
# filtered_data_with_sg = sg_filter.fit_transform(preprocessed_data_without_sg)
# residuals = filtered_data_with_sg - preprocessed_data_without_sg

# trimmed_wavenumbers = wavenumbers[:filtered_data_with_sg.shape[1]]

# plot_spectra(preprocessed_data_without_sg, trimmed_wavenumbers, label_np, chartTitle="preprocessed_data_without_sg Reflectance")
# plot_spectra(filtered_data_with_sg, trimmed_wavenumbers, label_np, chartTitle="Savitzky-Golay Reflectance")
# SG_spectra_preprocessed = filtered_data_with_sg



# # Plot the randomly selected samples
# Randomly select 5 sample indices
# random_indices = random.sample(range(preprocessed_data_without_sg.shape[0]), 5)

# plt.figure(figsize=(20, 15))

# for i, idx in enumerate(random_indices):
#     plt.subplot(5, 1, i + 1)
#     plt.plot(trimmed_wavenumbers, preprocessed_data_without_sg[idx], label=f"preprocessed_data_without_sg (Sample {idx + 1})", alpha=0.7, color="blue")
#     plt.plot(trimmed_wavenumbers, filtered_data_with_sg[idx], label=f"filtered_data_with_sg (Sample {idx + 1})", alpha=0.7, color="green")
#     plt.plot(trimmed_wavenumbers, residuals[idx], label=f"Residuals (Sample {idx + 1})", linestyle="--", alpha=0.7, color="red")
#     plt.xlabel("Wavenumbers")
#     plt.ylabel("Intensity")
#     plt.legend()
#     plt.title(f"Sample {idx + 1}")

# plt.tight_layout()
# plt.show()

# # Plot all the samples
# # Set the number of samples per figure
# samples_per_figure = 4
# num_samples = preprocessed_data_without_sg.shape[0]  # Total number of samples
# num_figures = (num_samples + samples_per_figure - 1) // samples_per_figure  # Total number of figures needed

# # Loop through the data in chunks of 4 samples per figure
# for fig_num in range(num_figures):
#     plt.figure(figsize=(20, 15))  # Set figure size to 20x15

#     # Get the range of samples for this figure
#     start_idx = fig_num * samples_per_figure
#     end_idx = min((fig_num + 1) * samples_per_figure, num_samples)  # Avoid out-of-bound indices

#     for i, sample_idx in enumerate(range(start_idx, end_idx)):
#         plt.subplot(samples_per_figure, 1, i + 1)
#         plt.plot(trimmed_wavenumbers, preprocessed_data_without_sg[sample_idx], label=f"preprocessed_data_without_sg (Sample {sample_idx + 1})", alpha=0.7, color="blue")
#         plt.plot(trimmed_wavenumbers, filtered_data_with_sg[sample_idx], label=f"filtered_data_with_sg (Sample {sample_idx + 1})", alpha=0.7, color="green")
#         plt.plot(trimmed_wavenumbers, residuals[sample_idx], label=f"Residuals (Sample {sample_idx + 1})", linestyle="--", alpha=0.7, color="red")
#         plt.xlabel("Wavenumbers")
#         plt.ylabel("Intensity")
#         plt.legend()
#         plt.title(f"Sample {sample_idx + 1}")

#     plt.tight_layout()
#     plt.show()


# plot the preprocessed spectra

# fig, ax = plt.subplots(figsize=(10, 4))
# ax.plot(param_grid['n_components'], np.abs(grid_search.cv_results_['mean_test_score']), marker='o', color='b')
# ax.set_xlabel('Number of components')
# ax.set_ylabel('Mean absolute error')
# ax.set_title('Cross validation results')

"""## Mean Filter"""

# create a pipeline that scales the data
MF_preprocessing = make_pipeline(
    RangeCut(start=400, end=1000, wavenumbers=wavenumbers),
    LinearCorrection(),
    MeanFilter(window_size=5),
    StandardScaler(with_std=False)
)

MF_spectra_preprocessed = MF_preprocessing.fit_transform(spectra_np)

# get the wavenumbers after the range cut
wavenumbers_cut = MF_preprocessing.named_steps['rangecut'].wavenumbers_
# plot the preprocessed spectra
plot_spectra(MF_spectra_preprocessed, wavenumbers_cut, label_np, chartTitle="Mean Filtered Reflectance")


''' ========================== Split the training data and testing data here ========================================= '''
from sklearn.model_selection import train_test_split
# Original Data
X_train_o, X_test_o, y_train_o, y_test_o = train_test_split(spectra_np, label_np, test_size=0.2, random_state=42)

# SG Data

X_train, X_test, y_train, y_test = train_test_split(SG_spectra_preprocessed, label_np, test_size=0.2, random_state=42)


#Mean Fileter Data
X_train_m, X_test_m, y_train_m, y_test_m = train_test_split(MF_spectra_preprocessed, label_np, test_size=0.2, random_state=42)


''' ================================================================================================================ '''




''' original dataset '''
print("====================== PLSRegression with original dataset ===========================")
# instanciate a PLSRegression object
pls = PLSRegression(scale=False)

param_grid = {'n_components': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]}

grid_search = GridSearchCV(pls, param_grid, cv=10, scoring='neg_mean_absolute_error')

# MF_spectra_preprocessed: HSI_spectra_124.xlsx => Spectra => spectra_np => MF_spectra_preprocessed
grid_search.fit(X_train_o, y_train_o)

# print the best parameters and score
print("Best parameters: ", grid_search.best_params_)
print("Best score: ", np.abs(grid_search.best_score_))

# instanciate a PLSRegression object with 10 components
pls = PLSRegression(n_components=10, scale=False)

# fit the model to the data
pls.fit(X_train_o, y_train_o)
# predict the Brix
label_pred = pls.predict(X_test_o)

# plot
fig, ax = plt.subplots(figsize=(4, 4))
ax.scatter(y_test_o, label_pred, color='blue')
ax.plot([7, 15], [7, 15], color='magenta')
ax.set_xlabel('Measured Brix')
ax.set_ylabel('Predicted Brix')
ax.set_title('original PLSR')


from sklearn.metrics import r2_score, mean_squared_error
print("====================== polyfit with original dataset ===========================")
# Fit a linear trendline
z = np.polyfit(y_test_o.ravel(), label_pred.ravel(), 1)
p = np.poly1d(z)
ax.plot(y_test_o, p(y_test_o), color='red', label='Trendline')

# Calculate R² and MSE
r2 = r2_score(y_test_o, label_pred)
mse = mean_squared_error(y_test_o, label_pred)

# Add R² value to the plot
ax.text(0.05, 0.95, f'R²: {r2:.3f}', transform=ax.transAxes,
        fontsize=10, verticalalignment='top', bbox=dict(boxstyle='round', facecolor='white', alpha=0.5))

# Print the metrics
print(f"R²: {r2:.3f}")
print(f"MSE: {mse:.3f}")
variance_brix = np.var(y_test_o)
print(f"Variance of Brix: {variance_brix:.3f}")

plt.show()

min_label = np.min(y_test_o)
max_label = np.max(y_test_o)
mean_label = np.mean(y_test_o)
std_label = np.std(y_test_o)

# Print the values
print(f"Min measured Brix: {min_label:.3f}")
print(f"Max measured Brix: {max_label:.3f}")
print(f"Mean measured Brix: {mean_label:.3f}")
print(f"Standard deviation of measured Brix: {std_label:.3f}")

# Convert predicted labels to a NumPy array
label_pred = np.array(label_pred)

# Calculate min, max, mean, and standard deviation
min_pred = np.min(label_pred)
max_pred = np.max(label_pred)
mean_pred = np.mean(label_pred)
std_pred = np.std(label_pred)

# Print the values
print(f"Min predicted Brix: {min_pred:.3f}")
print(f"Max predicted Brix: {max_pred:.3f}")
print(f"Mean predicted Brix: {mean_pred:.3f}")
print(f"Standard deviation of predicted Brix: {std_pred:.3f}")




"""# Prediction

Testing different methods and troubleshooting any potential issues.

## PLSR
Testing PLSR to see what works and compare SG to MF.

### SG Preprocessed
"""
print("====================== PLSRegression with SG_spectra_preprocessed dataset ===========================")
# instanciate a PLSRegression object
pls = PLSRegression(scale=False)

param_grid = {'n_components': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]}

grid_search = GridSearchCV(pls, param_grid, cv=10, scoring='neg_mean_absolute_error')

''' ========================== USeing Training data to train, testing data to test ================================ '''
grid_search.fit(X_train, y_train)

# print the best parameters and score
print("Best parameters: ", grid_search.best_params_)
print("Best score: ", np.abs(grid_search.best_score_))

''' ========================== USeing best model to predict testing set ================================ '''

# instanciate a PLSRegression object with 10 components
pls = PLSRegression(n_components=10, scale=False)

# fit the model to the data
pls.fit(X_train, y_train)

# predict the Brix
# SG_label_pred = pls.predict(SG_spectra_preprocessed)

SG_label_pred = pls.predict(X_test)

# plot the predictions
fig, ax = plt.subplots(figsize=(4, 4))
ax.scatter(y_test, SG_label_pred, color='blue')

ax.plot([7, 15], [7, 15], color='magenta')
ax.set_xlabel('Measured Brix')
ax.set_ylabel('Predicted Brix')
ax.set_title('Savitzky-Golay PLSR')



from sklearn.metrics import r2_score, mean_squared_error

# Fit a linear trendline
z = np.polyfit(y_test.ravel(), SG_label_pred.ravel(), 1)
p = np.poly1d(z)
ax.plot(y_test, p(y_test), color='red', label='Trendline')

# Calculate R² and MSE
SG_r2 = r2_score(y_test, SG_label_pred)
SG_mse = mean_squared_error(y_test, SG_label_pred)

# Add R² value to the plot
ax.text(0.05, 0.95, f'R²: {SG_r2:.3f}', transform=ax.transAxes,
        fontsize=10, verticalalignment='top', bbox=dict(boxstyle='round', facecolor='white', alpha=0.5))

# Print the metrics
print(f"R²: {SG_r2:.3f}")
print(f"MSE: {SG_mse:.3f}")
variance_brix = np.var(y_test)
print(f"Variance of Brix: {variance_brix:.3f}")
plt.show()

min_label = np.min(y_test)
max_label = np.max(y_test)
mean_label = np.mean(y_test)
std_label = np.std(y_test)

# Print the values
print(f"Min measured Brix: {min_label:.3f}")
print(f"Max measured Brix: {max_label:.3f}")
print(f"Mean measured Brix: {mean_label:.3f}")
print(f"Standard deviation of measured Brix: {std_label:.3f}")

# Convert predicted labels to a NumPy array
SG_label_pred = np.array(SG_label_pred)

# Calculate min, max, mean, and standard deviation
min_pred = np.min(SG_label_pred)
max_pred = np.max(SG_label_pred)
mean_pred = np.mean(SG_label_pred)
std_pred = np.std(SG_label_pred)

# Print the values
print(f"Min predicted Brix: {min_pred:.3f}")
print(f"Max predicted Brix: {max_pred:.3f}")
print(f"Mean predicted Brix: {mean_pred:.3f}")
print(f"Standard deviation of predicted Brix: {std_pred:.3f}")


'''========================= same method with MF_spectra_preprocessed dataset ========================'''
"""### Mean Filter Preprocessed"""
print("====================== PLSRegression with MF_spectra_preprocessed dataset ===========================")
# instanciate a PLSRegression object
pls = PLSRegression(scale=False)

param_grid = {'n_components': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]}

grid_search = GridSearchCV(pls, param_grid, cv=10, scoring='neg_mean_absolute_error')

# MF_spectra_preprocessed: HSI_spectra_124.xlsx => Spectra => spectra_np => MF_spectra_preprocessed
# grid_search.fit(MF_spectra_preprocessed, label_np)
grid_search.fit(X_train_m, y_train_m)

# print the best parameters and score
print("Best parameters: ", grid_search.best_params_)
print("Best score: ", np.abs(grid_search.best_score_))


# # instanciate a PLSRegression object with 10 components
pls = PLSRegression(n_components=10, scale=False)

# # fit the model to the data
pls.fit(X_train_m, y_train_m)
# # predict the Brix
MF_label_pred = pls.predict(X_test_m)


# plot
fig, ax = plt.subplots(figsize=(4, 4))
ax.scatter(y_test_m, MF_label_pred, color='blue')
ax.plot([7, 15], [7, 15], color='magenta')
ax.set_xlabel('Measured Brix')
ax.set_ylabel('Predicted Brix')
ax.set_title('Mean Filtered PLSR')




from sklearn.metrics import r2_score, mean_squared_error

# Fit a linear trendline
z = np.polyfit(y_test_m.ravel(), MF_label_pred.ravel(), 1)
p = np.poly1d(z)
ax.plot(y_test_m, p(y_test_m), color='red', label='Trendline')

# Calculate R² and MSE
r2 = r2_score(y_test_m, MF_label_pred)
mse = mean_squared_error(y_test_m, MF_label_pred)

# Add R² value to the plot
ax.text(0.05, 0.95, f'R²: {r2:.3f}', transform=ax.transAxes,
        fontsize=10, verticalalignment='top', bbox=dict(boxstyle='round', facecolor='white', alpha=0.5))

# Print the metrics
print(f"R²: {r2:.3f}")
print(f"MSE: {mse:.3f}")
variance_brix = np.var(y_test_m)
print(f"Variance of Brix: {variance_brix:.3f}")

plt.show()

min_label = np.min(y_test_m)
max_label = np.max(y_test_m)
mean_label = np.mean(y_test_m)
std_label = np.std(y_test_m)

# Print the values
print(f"Min measured Brix: {min_label:.3f}")
print(f"Max measured Brix: {max_label:.3f}")
print(f"Mean measured Brix: {mean_label:.3f}")
print(f"Standard deviation of measured Brix: {std_label:.3f}")

# Convert predicted labels to a NumPy array
MF_label_pred = np.array(MF_label_pred)

# Calculate min, max, mean, and standard deviation
min_pred = np.min(MF_label_pred)
max_pred = np.max(MF_label_pred)
mean_pred = np.mean(MF_label_pred)
std_pred = np.std(MF_label_pred)

# Print the values
print(f"Min predicted Brix: {min_pred:.3f}")
print(f"Max predicted Brix: {max_pred:.3f}")
print(f"Mean predicted Brix: {mean_pred:.3f}")
print(f"Standard deviation of predicted Brix: {std_pred:.3f}")

""" ## SVM/R### SG Pre """

# Instantiate an SVR model
svr = SVR()

# Define the parameter grid for SVR
param_grid = {'C': [0.1, 1, 10, 100], 'epsilon': [0.01, 0.1, 1]}

# Create a grid search object
grid_search_svr = GridSearchCV(svr, param_grid, cv=10, scoring='neg_mean_absolute_error')

# Use ravel() to convert label_np to a 1D array
# label_np_1d = label_np.ravel()
'''====================== transform training and testing label to 1D array ======================'''
label_np_1d = label_np.ravel()
y_train_1d = y_train.ravel()
y_test_1d = y_test.ravel()
y_train_o_1d = y_train_o.ravel()
y_test_o_1d = y_test_o.ravel()


# Now fit the SVR model
grid_search_svr.fit(X_train, y_train_1d)

# Predict the Brix levels using the best SVR model
# SG_label_pred_svr = grid_search_svr.best_estimator_.predict(X_test)
svr = SVR()
svr.fit(X_train, y_train_1d)
SG_label_pred_svr = svr.predict(X_test)

# Plot the predictions (similar to PLSR)
plt.scatter(y_test_1d, SG_label_pred_svr, color='blue')
plt.plot([7, 15], [7, 15], color='magenta')
plt.xlabel('Measured Brix')
plt.ylabel('Predicted Brix')
plt.title('SVR Predictions')
plt.text(7.2, 14.8, f"$R^2$: {r2_score(y_test_1d, SG_label_pred_svr):.3f}", fontsize=12, color='black')  # Add R² score at the top left corner
plt.legend()
plt.show()

# Evaluate the model
print("Best parameters for SVR: ", grid_search_svr.best_params_)
print(f"R²: {r2_score(y_test_1d, SG_label_pred_svr):.3f}")
print(f"MSE: {mean_squared_error(y_test_1d, SG_label_pred_svr):.3f}")


print('fit with original data')
# Now fit the SVR model
grid_search_svr.fit(X_train_o, y_train_o_1d)

# Predict the Brix levels using the best SVR model
# label_pred_svr = grid_search_svr.best_estimator_.predict(X_test_o)
svr = SVR()
svr.fit(X_train_o, y_train_o_1d)
label_pred_svr = svr.predict(X_test_o)

# Plot the predictions (similar to PLSR)
plt.scatter(y_test_o_1d, label_pred_svr, color='blue')
plt.plot([7, 15], [7, 15], color='magenta')
plt.xlabel('Measured Brix')
plt.ylabel('Predicted Brix')
plt.title('SVR Predictions with original data')
plt.text(7.2, 14.8, f"$R^2$: {r2_score(y_test_o_1d, label_pred_svr):.3f}", fontsize=12, color='black')  # Add R² score at the top left corner
plt.show()

# Evaluate the model
print("Best parameters for SVR: ", grid_search_svr.best_params_)
print(f"R²: {r2_score(y_test_o_1d, label_pred_svr):.3f}")
print(f"MSE: {mean_squared_error(y_test_o_1d, label_pred_svr):.3f}")




"""## Random Forest"""

# # Convert label_np to 1D array using ravel()
# label_np_1d = label_np.ravel()

# Define the RandomForest model
rf = RandomForestRegressor(random_state=42)

''' =============================== 10 fold cross validation ========================='''

# Define the cross-validation strategy
cv = KFold(n_splits=10, shuffle=True, random_state=42)  # 10-fold CV


r2_scores = cross_val_score(rf, X_train, y_train_1d, cv=cv, scoring='r2')

# Perform cross-validation and get MSE scores (convert negative values to positive)
mse_scores = -cross_val_score(rf, X_train, y_train_1d, cv=cv, scoring='neg_mean_squared_error')

# Print cross-validated performance
print(f"Mean R² across folds: {np.mean(r2_scores):.3f} ± {np.std(r2_scores):.3f}")
print(f"Mean MSE across folds: {np.mean(mse_scores):.3f} ± {np.std(mse_scores):.3f}")

''' =============================== not 10 fold cross validation ========================'''
# Fit Random Forest on dataset
rf.fit(X_train, y_train_1d)

# Predict on the entire dataset
SG_label_pred_rf = rf.predict(X_test)

# Plot
plt.scatter(y_test_1d, SG_label_pred_rf, color='green')
plt.plot([7, 15], [7, 15], color='magenta')
plt.xlabel('Measured Brix')
plt.ylabel('Predicted Brix')
plt.title('Random Forest Predictions (Full Data)')
plt.text(7.2, 14.8, f"$R^2$: {np.mean(r2_scores):.3f} ± {np.std(r2_scores):.3f}", fontsize=12, color='black')  # Add R² score at the top left corner
plt.show()



print('fit with original data')
r2_scores = cross_val_score(rf, X_train_o, y_train_o_1d, cv=cv, scoring='r2')

# Perform cross-validation and get MSE scores (convert negative values to positive)
mse_scores = -cross_val_score(rf, X_train_o, y_train_o_1d, cv=cv, scoring='neg_mean_squared_error')

# Print cross-validated performance
print(f"Mean R² across folds: {np.mean(r2_scores):.3f} ± {np.std(r2_scores):.3f}")
print(f"Mean MSE across folds: {np.mean(mse_scores):.3f} ± {np.std(mse_scores):.3f}")

# Fit Random Forest on dataset
rf.fit(X_train_o, y_train_o_1d)

# Predict on the entire dataset
label_pred_rf = rf.predict(X_test_o)

# Plot
plt.scatter(y_test_o_1d, label_pred_rf, color='green')
plt.plot([7, 15], [7, 15], color='magenta')
plt.xlabel('Measured Brix')
plt.ylabel('Predicted Brix')
plt.title('Random Forest Predictions (original data)')
plt.text(7.2, 14.8, f"$R^2$: {np.mean(r2_scores):.3f} ± {np.std(r2_scores):.3f}", fontsize=12, color='black')  # Add R² score at the top left corner
plt.show()




"""## ANN"""

# Define the parameter grid for tuning
param_grid = {
    'hidden_layer_sizes': [(50,), (100,), (100, 50), (100, 100, 50)],  # Different layer sizes
    'activation': ['relu', 'tanh', 'logistic'],  # Activation functions
    'solver': ['adam', 'lbfgs', 'sgd'],  # Optimizers
    'alpha': [0.0001, 0.001, 0.01],  # Regularization strength
    'learning_rate': ['constant', 'adaptive'],  # Learning rate schedule
    'max_iter': [500, 1000, 2000]  # Maximum number of iterations
}

# Define the MLPRegressor model
mlp = MLPRegressor(random_state=42)

# Define the cross-validation strategy (10-fold CV)
cv = KFold(n_splits=10, shuffle=True, random_state=42)

# Perform RandomizedSearchCV to tune the hyperparameters
random_search = RandomizedSearchCV(mlp, param_distributions=param_grid,
                                    n_iter=20, cv=cv, scoring='r2', n_jobs=-1, random_state=42)

# Fit the randomized search to data
random_search.fit(X_train, y_train_1d)

# Print the best parameters and the corresponding R² score
print(f"Best Parameters: {random_search.best_params_}")
print(f"Best R²: {random_search.best_score_:.3f}")

# get the best model
best_mlp_model = random_search.best_estimator_

# Predict with the best model
# predicted_brix = best_mlp_model.predict(X_test)
mlp = MLPRegressor(random_state=42)
mlp.fit(X_train, y_train_1d)
predicted_brix = mlp.predict(X_test)

# Plot actual vs predicted values
import matplotlib.pyplot as plt
plt.scatter(y_test_1d, predicted_brix, color='blue')
plt.plot([min(y_test_1d), max(y_test_1d)], [min(y_test_1d), max(y_test_1d)], color='magenta')
plt.xlabel('Measured Brix')
plt.ylabel('Predicted Brix')
plt.title('MLP Neural Network Predictions (Best Model)')
plt.text(7.2, 14.8, f"$R^2$: {random_search.best_score_:.3f}", fontsize=12, color='black')  # Add R² score at the top left corner
plt.show()


print('fit with original data')
# Fit the randomized search to data
random_search.fit(X_train_o, y_train_o_1d)

# Print the best parameters and the corresponding R² score
print(f"Best Parameters: {random_search.best_params_}")
print(f"Best R²: {random_search.best_score_:.3f}")

# get the best model
best_mlp_model = random_search.best_estimator_

# Predict with the best model
# predicted_brix = best_mlp_model.predict(X_test_o)
mlp = MLPRegressor(random_state=42)
mlp.fit(X_train_o, y_train_o_1d)
predicted_brix = mlp.predict(X_test_o)

# Plot actual vs predicted values
import matplotlib.pyplot as plt
plt.scatter(y_test_o_1d, predicted_brix, color='blue')
plt.plot([min(y_test_o_1d), max(y_test_o_1d)], [min(y_test_o_1d), max(y_test_o_1d)], color='magenta')
plt.xlabel('Measured Brix')
plt.ylabel('Predicted Brix')
plt.title('MLP Neural Network Predictions (Best Model with original data)')
plt.text(7.2, 14.8, f"$R^2$: {random_search.best_score_:.3f}", fontsize=12, color='black')  # Add R² score at the top left corner
plt.show()


"""## KNN"""

# Define the KNN Regressor model
knn = KNeighborsRegressor(n_neighbors=5)

# # Convert label_np to 1D array
# label_np_1d = label_np.ravel()

cv = KFold(n_splits=10, shuffle=True, random_state=42)

# Perform cross-validation and get R² scores
r2_scores = cross_val_score(knn, X_train, y_train_1d, cv=cv, scoring='r2')

# Perform cross-validation and get MSE scores
mse_scores = cross_val_score(knn, X_train, y_train_1d, cv=cv, scoring='neg_mean_squared_error')
mse_scores = -mse_scores  # Convert negative MSE values to positive

# Print the cross-validated performance
print(f"Mean R² across folds: {np.mean(r2_scores):.3f} ± {np.std(r2_scores):.3f}")
print(f"Mean MSE across folds: {np.mean(mse_scores):.3f} ± {np.std(mse_scores):.3f}")


knn.fit(X_train, y_train_1d)

predicted_brix = knn.predict(X_test)

# Plot actual vs predicted values
import matplotlib.pyplot as plt
plt.scatter(y_test_1d, predicted_brix, color='blue')
plt.plot([min(y_test_1d), max(y_test_1d)], [min(y_test_1d), max(y_test_1d)], color='magenta')
plt.xlabel('Measured Brix')
plt.ylabel('Predicted Brix')
plt.title('KNN Regression Predictions')
plt.text(7.2, 14.8, f"$R^2$: {np.mean(r2_scores):.3f} ± {np.std(r2_scores):.3f}", fontsize=12, color='black')  # Add R² score at the top left corner
plt.show()



print('fit with original data')
# Perform cross-validation and get R² scores
r2_scores = cross_val_score(knn, X_train_o, y_train_o_1d, cv=cv, scoring='r2')

# Perform cross-validation and get MSE scores
mse_scores = cross_val_score(knn, X_train_o, y_train_o_1d, cv=cv, scoring='neg_mean_squared_error')
mse_scores = -mse_scores  # Convert negative MSE values to positive

# Print the cross-validated performance
print(f"Mean R² across folds: {np.mean(r2_scores):.3f} ± {np.std(r2_scores):.3f}")
print(f"Mean MSE across folds: {np.mean(mse_scores):.3f} ± {np.std(mse_scores):.3f}")

knn.fit(X_train_o, y_train_o_1d)

predicted_brix = knn.predict(X_test_o)

# Plot actual vs predicted values
import matplotlib.pyplot as plt
plt.scatter(y_test_o_1d, predicted_brix, color='blue')
plt.plot([min(y_test_o_1d), max(y_test_o_1d)], [min(y_test_o_1d), max(y_test_o_1d)], color='magenta')
plt.xlabel('Measured Brix')
plt.ylabel('Predicted Brix')
plt.title('KNN Regression Predictions(original data)')
plt.text(7.2, 14.8, f"$R^2$: {np.mean(r2_scores):.3f} ± {np.std(r2_scores):.3f}", fontsize=12, color='black')  # Add R² score at the top left corner
plt.show()



''' ========================================================================== '''

"""## XGBoost"""

# Define the XGBoost model
xgb = XGBRegressor(objective='reg:squarederror', random_state=42)

# # Convert label_np to 1D array
# label_np_1d = label_np.ravel()

cv = KFold(n_splits=10, shuffle=True, random_state=42)

# Perform cross-validation and get R² scores
r2_scores = cross_val_score(xgb, X_train, y_train_1d, cv=cv, scoring='r2')

# Perform cross-validation and get MSE scores
mse_scores = cross_val_score(xgb, X_train, y_train_1d, cv=cv, scoring='neg_mean_squared_error')
mse_scores = -mse_scores  # Convert negative MSE values to positive

# Print the cross-validated performance
print(f"Mean R² across folds: {np.mean(r2_scores):.3f} ± {np.std(r2_scores):.3f}")
print(f"Mean MSE across folds: {np.mean(mse_scores):.3f} ± {np.std(mse_scores):.3f}")

xgb.fit(X_train, y_train_1d)

predicted_brix = xgb.predict(X_test)

# Plot actual vs predicted values
import matplotlib.pyplot as plt
plt.scatter(y_test_1d, predicted_brix, color='blue')
plt.plot([min(y_test_1d), max(y_test_1d)], [min(y_test_1d), max(y_test_1d)], color='magenta')
plt.xlabel('Measured Brix')
plt.ylabel('Predicted Brix')
plt.title('XGBoost Predictions')
plt.text(7.2, 14.8, f"$R^2$: {np.mean(r2_scores):.3f} ± {np.std(r2_scores):.3f}", fontsize=12, color='black')  # Add R² score at the top left corner
plt.show()


print('fit with original data')
r2_scores = cross_val_score(xgb, X_train_o, y_train_o_1d, cv=cv, scoring='r2')

# Perform cross-validation and get MSE scores
mse_scores = cross_val_score(xgb, X_train_o, y_train_o_1d, cv=cv, scoring='neg_mean_squared_error')
mse_scores = -mse_scores  # Convert negative MSE values to positive

# Print the cross-validated performance
print(f"Mean R² across folds: {np.mean(r2_scores):.3f} ± {np.std(r2_scores):.3f}")
print(f"Mean MSE across folds: {np.mean(mse_scores):.3f} ± {np.std(mse_scores):.3f}")

xgb.fit(X_train_o, y_train_o_1d)

predicted_brix = xgb.predict(X_test_o)

# Plot actual vs predicted values
import matplotlib.pyplot as plt
plt.scatter(y_test_o_1d, predicted_brix, color='blue')
plt.plot([min(y_test_o_1d), max(y_test_o_1d)], [min(y_test_o_1d), max(y_test_o_1d)], color='magenta')
plt.xlabel('Measured Brix')
plt.ylabel('Predicted Brix')
plt.title('XGBoost Predictions(original data)')
plt.text(7.2, 14.8, f"$R^2$: {np.mean(r2_scores):.3f} ± {np.std(r2_scores):.3f}", fontsize=12, color='black')  # Add R² score at the top left corner
plt.show()


# %%
"""## Model Optimization

Now that I ironed out the compatibility issues, here is where actual model development starts.
"""

# Defining the parameters for each model
param_grids = {
    'PLSR': {'n_components': [2, 3, 4, 5, 6, 7, 8, 9, 10]},
    'SVR': {'C': [0.1, 1, 10, 100], 'epsilon': [0.01, 0.1, 1], 'kernel': ['linear', 'rbf']},
    'Random Forest': {'n_estimators': [50, 100, 200], 'max_depth': [None, 10, 20, 30]},
    'ANN': {
        'hidden_layer_sizes': [(50,), (100,), (100, 50), (100, 100, 50)],  # Different hidden layer sizes
        'activation': ['relu', 'tanh'],  # Activation functions
        'solver': ['adam', 'lbfgs'],  # Optimizers
        'alpha': [0.0001, 0.001, 0.01]  # Regularization strength
    },
    'KNN': {
        'n_neighbors': [3, 5, 7, 10],  # Different numbers of neighbors
        'weights': ['uniform', 'distance'],  # Weighting methods
        'metric': ['euclidean', 'manhattan']  # Distance metrics
    },
    'XGBoost': {
        'n_estimators': [50, 100, 200],  # Number of boosting rounds
        'max_depth': [3, 5, 7],  # Maximum depth of trees
        'learning_rate': [0.01, 0.1, 0.2],  # Learning rate for boosting
        'subsample': [0.7, 0.8, 1.0],  # Subsampling for each tree
        'colsample_bytree': [0.7, 0.8, 1.0]  # Subsampling for columns
    }
}

# Define models to compare without parameters
pls = PLSRegression(scale=False)
svr = SVR()
rf = RandomForestRegressor()
ann = MLPRegressor(max_iter=1000)
knn = KNeighborsRegressor()
xgb = XGBRegressor(objective='reg:squarederror', random_state=42)

models = {
    'PLSR': pls,
    'SVR': svr,
    'Random Forest': rf,
    'ANN': ann,
    'KNN': knn,
    'XGBoost': xgb
}

# # Convert label_np to 1D array
# label_np_1d = label_np.ravel()

# Define the cross-validation strategy
cv = KFold(n_splits=10, shuffle=True, random_state=42)  # 10-fold CV

# Parameter tuning and cross-validation for each model
results = {}

for model_name, model in models.items():
    print(f"Evaluating {model_name} with parameter tuning...")

    # Parameter tuning
    grid_search = GridSearchCV(model, param_grids[model_name], cv=cv, scoring='neg_mean_squared_error', n_jobs=-1)

    '''================================ potential data leakage ==================================='''
    '''============== here the dataset used for grid_search, cross_val_score and model.pridict are same =================='''
    # # Fit the grid search object to the data
    # grid_search.fit(SG_spectra_preprocessed, label_np_1d)
    
    grid_search.fit(X_train, y_train_1d)

    # Get the best model from the grid search
    best_model = grid_search.best_estimator_


    # # Evaluate the best model using cross-validation with R² score
    # r2_scores = cross_val_score(best_model, SG_spectra_preprocessed, label_np_1d, cv=cv, scoring='r2')
    # mse_scores = cross_val_score(best_model, SG_spectra_preprocessed, label_np_1d, cv=cv, scoring='neg_mean_squared_error')
    # mse_scores = -mse_scores  # Convert negative MSE values to positive
    r2_scores = cross_val_score(best_model, X_train, y_train_1d, cv=cv, scoring='r2')
    mse_scores = cross_val_score(best_model, X_train, y_train_1d, cv=cv, scoring='neg_mean_squared_error')
    mse_scores = -mse_scores  # Convert negative MSE values to positive

    results[model_name] = {
        'Best Params': grid_search.best_params_,
        'MSE Mean': np.mean(mse_scores),
        'MSE Std': np.std(mse_scores),
        'R² Mean': np.mean(r2_scores),
        'R² Std': np.std(r2_scores)
    }

# Print the results
for model_name, result in results.items():
    print(f"\nModel: {model_name}")
    print(f"Best Parameters: {result['Best Params']}")
    print(f"Mean MSE: {result['MSE Mean']:.3f} ± {result['MSE Std']:.3f}")
    print(f"Mean R²: {result['R² Mean']:.3f} ± {result['R² Std']:.3f}")

# importances = rf.feature_importances_
# indices = np.argsort(importances)[::-1]  # Sort by importance

# # Plot the top features
# plt.figure(figsize=(10, 6))
# plt.bar(range(10), importances[indices[:10]], color="r", align="center")
# plt.xticks(range(10), indices[:10])
# plt.title('Top 10 Feature Importances (Random Forest)')
# plt.xlabel('Feature Index')
# plt.ylabel('Importance Score')
# plt.show()

"""# Final Model figures"""

## Training the final models
# Retrain each model using the best parameters from the grid search
final_models = {}

for model_name, result in results.items():
    best_params = result['Best Params']

    if model_name == 'PLSR':
        final_model = PLSRegression(**best_params)
    elif model_name == 'SVR':
        final_model = SVR(**best_params)
    elif model_name == 'Random Forest':
        final_model = RandomForestRegressor(**best_params)
    elif model_name == 'ANN':
        final_model = MLPRegressor(max_iter=1000, **best_params)
    elif model_name == 'KNN':
        final_model = KNeighborsRegressor(**best_params)
    elif model_name == 'XGBoost':
        final_model = XGBRegressor(objective='reg:squarederror', random_state=42, **best_params)

    '''================================ fit trainingset, predict testing set =========================='''
    '''========================already used gridsearch to get the best model, why use model with best parameter 
                                        to train the training set one more time============================'''
    # # Fit the final model to the full dataset
    # final_model.fit(SG_spectra_preprocessed, label_np_1d)
    final_model.fit(X_train, y_train_1d)
    final_models[model_name] = final_model

# # Predict using the final models
# for model_name, final_model in final_models.items():
#     predictions = final_model.predict(SG_spectra_preprocessed)

#     print(f"{model_name} predictions: {predictions[:5]}")  # Print first 5 predictions as an example



# Predictions for each model
predicted_brix_plsr = final_models['PLSR'].predict(X_test)
predicted_brix_svr = final_models['SVR'].predict(X_test)
predicted_brix_rf = final_models['Random Forest'].predict(X_test)
predicted_brix_ann = final_models['ANN'].predict(X_test)
predicted_brix_knn = final_models['KNN'].predict(X_test)
predicted_brix_xgb = final_models['XGBoost'].predict(X_test)

# # Plotting actual vs predicted, add trendline, and R²
# def plot_with_trendline(ax, actual, predicted, model_name, color):
#     ax.scatter(actual, predicted, color=color)
#     ax.plot([min(actual), max(actual)], [min(actual), max(actual)], color='magenta', linestyle='--')

#     # Fit a linear trendline
#     z = np.polyfit(actual, predicted, 1)  # Fit a 1st degree polynomial (straight line)
#     p = np.poly1d(z)
#     ax.plot(actual, p(actual), color='red', label='Trendline')  # Add the trendline

#     # Calculate R²
#     r2 = r2_score(actual, predicted)
#     ax.text(0.05, 0.90, f'R²: {r2:.3f}', transform=ax.transAxes, fontsize=10,
#             verticalalignment='top', bbox=dict(boxstyle='round', facecolor='white', alpha=0.5))

# # Title and axis labels with increased fontsize
#     ax.set_title(f'{model_name}: Predicted Brix', fontsize=16)
#     ax.set_xlabel('Measured 6-Methyl-5-Hepten-2-one', fontsize=14)
#     ax.set_ylabel('Predicted 6-Methyl-5-Hepten-2-one', fontsize=14)

#     # Move legend
#     ax.legend(loc='upper left', fontsize=12)

# fig, axs = plt.subplots(3, 2, figsize=(12, 18))

# # PLSR
# plot_with_trendline(axs[0, 0], actual_brix, predicted_brix_plsr, 'PLSR', 'blue')

# # SVR
# plot_with_trendline(axs[0, 1], actual_brix, predicted_brix_svr, 'SVR', 'green')

# # Random Forest
# plot_with_trendline(axs[1, 0], actual_brix, predicted_brix_rf, 'Random Forest', 'orange')

# # ANN
# plot_with_trendline(axs[1, 1], actual_brix, predicted_brix_ann, 'ANN', 'purple')

# # KNN
# plot_with_trendline(axs[2, 0], actual_brix, predicted_brix_knn, 'KNN', 'red')

# # XGBoost
# plot_with_trendline(axs[2, 1], actual_brix, predicted_brix_xgb, 'XGBoost', 'cyan')

# plt.tight_layout()

# # Show the combined figure
# plt.show()

# Plotting actual vs predicted, add trendline, and R²
def plot_with_trendline(actual, predicted, model_name, color):
    plt.figure(figsize=(5, 5))  # Create a new figure for each model
    plt.scatter(actual, predicted, color=color)
    plt.plot([min(actual), max(actual)], [min(actual), max(actual)], color='magenta', linestyle='--')

    # Fit a linear trendline
    z = np.polyfit(actual, predicted, 1)  # Fit a 1st degree polynomial (straight line)
    p = np.poly1d(z)
    plt.plot(actual, p(actual), color='red', label='Trendline')  # Add the trendline

    # Calculate R²
    r2 = r2_score(actual, predicted)
    plt.text(0.05, 0.85, f'R²: {r2:.3f}', transform=plt.gca().transAxes, fontsize=10,
              verticalalignment='top', bbox=dict(boxstyle='round', facecolor='white', alpha=0.5))

    # Title and axis labels with increased fontsize
    plt.title(f'{model_name}: Predicted Brix-124', fontsize=16)
    plt.xlabel('Measured Brix', fontsize=14)
    plt.ylabel('Predicted Brix', fontsize=14)

    # Move legend
    plt.legend(loc='upper left', fontsize=12)

    # Show the figure
    plt.tight_layout()
    plt.show()
'''=======================can't use same dataset for training and testing========================'''
# actual_brix = label_np.ravel()
actual_brix = y_test_1d
# Plot each model separately
plot_with_trendline(actual_brix, predicted_brix_plsr, 'PLSR', 'blue')
plot_with_trendline(actual_brix, predicted_brix_svr, 'SVR', 'green')
plot_with_trendline(actual_brix, predicted_brix_rf, 'Random Forest', 'orange')
plot_with_trendline(actual_brix, predicted_brix_ann, 'ANN', 'purple')
plot_with_trendline(actual_brix, predicted_brix_knn, 'KNN', 'red')
plot_with_trendline(actual_brix, predicted_brix_xgb, 'XGBoost', 'cyan')


# %%
"""## Feature Importance"""

def plot_feature_importance(final_models, model_name, x_labels):
    # Validate that the model exists and has feature_importances_
    if model_name not in final_models:
        raise ValueError(f"Model '{model_name}' not found in final_models.")
    feature_importances = final_models[model_name].feature_importances_
    
    # Validate x_labels length
    if len(x_labels) != len(feature_importances):
        raise ValueError("The length of x_labels must match the number of features in the model.")

    # Ensure x_labels is a NumPy array
    x_labels = np.array(x_labels)
    
    # Plot the feature importances
    plt.figure(figsize=(10, 6))
    plt.bar(x_labels, feature_importances, color='green')
    plt.title(f'{model_name} Feature Importance')
    plt.xlabel('Wavelength (nm)')
    plt.ylabel('Importance Score')
    plt.xticks(rotation=45)  # Rotate x-tick labels for clarity
    plt.show()

    # Sort the feature importances and their corresponding wavelengths
    sorted_indices = np.argsort(feature_importances)[::-1]  # Indices for descending order
    sorted_importances = feature_importances[sorted_indices]  # Sorted importance scores
    sorted_x_labels = x_labels[sorted_indices].astype(str)  # Corresponding sorted wavelengths
    
    # Select the top N features
    top_n = 20
    sorted_x_labels_top = sorted_x_labels[:top_n]
    sorted_importances_top = sorted_importances[:top_n]
    
    # Plot only the top N features
    plt.figure(figsize=(10, 6))
    plt.bar(sorted_x_labels_top, sorted_importances_top, color='green')
    plt.title(f'{model_name} Feature Importance (Top {top_n})')
    plt.xlabel('Wavelength (nm)')
    plt.ylabel('Importance Score')
    
    # Explicitly set tick labels and adjust rotation for better readability
    plt.xticks(rotation=90, fontsize=10)  # Rotate x-ticks
    plt.show()
    
x_labels = wavenumbers_cut

for model_name in ['PLSR', 'SVR', 'Random Forest', 'ANN', 'KNN', 'XGBoost']:
    try:
        plot_feature_importance(final_models, model_name, x_labels)
    except AttributeError as e:
        print(f"Skipping {model_name}: {e}")
        
from sklearn.inspection import permutation_importance

def calculate_permutation_importance(model, X, y):
    result = permutation_importance(model, X, y, scoring='r2', n_repeats=10, random_state=42)
    return result.importances_mean





# # # Calculate permutation importance for SVR
# # result_svr = permutation_importance(final_models['SVR'], SG_spectra_preprocessed, label_np_1d, n_repeats=10, random_state=42)

# # # Extract the importance scores
# # feature_importances_svr = result_svr.importances_mean

# # x_labels = np.arange(400, 999.4, 2.7)

# # # Plot the feature importances for SVR
# # plt.figure(figsize=(10, 6))
# # plt.bar(x_labels, feature_importances_svr, color='purple')
# # plt.title('SVR Feature Importance (Permutation Importance)')
# # plt.xlabel('Wavelength (nm)')
# # plt.ylabel('Importance Score')
# # plt.xticks(rotation=45)  # Rotate x-tick labels for clarity
# # plt.show()

# # # Set up a 2x3 grid of subplots (2 rows, 3 columns)
# # fig, axs = plt.subplots(3, 2, figsize=(12, 18))  # Adjust figsize for better layout

# # # Function to plot error distribution for a given model in a specific axis
# # def plot_error_distribution(ax, model_name, model, X, y_true):
# #     predictions = model.predict(X)
# #     errors = y_true - predictions

# #     # Plot error distribution in the specific subplot (ax)
# #     sns.histplot(errors, kde=True, color='blue', ax=ax)
# #     ax.set_title(f'{model_name} Error Distribution', fontsize = 16)
# #     ax.set_xlabel('Prediction Error (Brix)')
# #     ax.set_ylabel('Density')

# # # Model names for the subplots
# # model_names = ['PLSR', 'SVR', 'Random Forest', 'ANN', 'KNN', 'XGBoost']

# # # Plot each model's error distribution in the corresponding subplot
# # for i, model_name in enumerate(model_names):
# #     row = i // 2  # Calculate row position (0 or 1)
# #     col = i % 2   # Calculate column position (0, 1, or 2)
# #     plot_error_distribution(axs[row, col], model_name, final_models[model_name], MF_spectra_preprocessed, label_np_1d)

# # # Adjust layout to prevent overlap
# # plt.tight_layout()

# # # Show the combined figure with all error distributions
# # plt.show()